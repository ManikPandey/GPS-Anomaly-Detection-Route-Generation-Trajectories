{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7688d4",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    " Import all necessary libraries for data manipulation, geospatial analysis, machine learning, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c043d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting folium\n",
      "  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting shapely\n",
      "  Downloading shapely-2.1.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Downloading branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (2.1.3)\n",
      "Requirement already satisfied: requests in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (2.32.3)\n",
      "Collecting xyzservices (from folium)\n",
      "  Downloading xyzservices-2025.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manik\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (2025.1.31)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Downloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
      "Downloading shapely-2.1.1-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Downloading xyzservices-2025.4.0-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: xyzservices, shapely, geographiclib, geopy, branca, folium\n",
      "\n",
      "   ---------------------------------------- 0/6 [xyzservices]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------ --------------------------------- 1/6 [shapely]\n",
      "   ------------- -------------------------- 2/6 [geographiclib]\n",
      "   ------------- -------------------------- 2/6 [geographiclib]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------- ------------------- 3/6 [geopy]\n",
      "   -------------------------- ------------- 4/6 [branca]\n",
      "   -------------------------- ------------- 4/6 [branca]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   --------------------------------- ------ 5/6 [folium]\n",
      "   ---------------------------------------- 6/6 [folium]\n",
      "\n",
      "Successfully installed branca-0.8.1 folium-0.20.0 geographiclib-2.0 geopy-2.4.1 shapely-2.1.1 xyzservices-2025.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy folium shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd34eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.cluster import DBSCAN\n",
    "import folium\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf91403",
   "metadata": {},
   "source": [
    "### 2. Configuration and Constants\n",
    "Define paths and parameters that will be used throughout the notebook. This makes the code cleaner and easier to modify.\n",
    "\n",
    " --- Configuration ---\n",
    " IMPORTANT: This assumes your notebook is in the root folder, and the data is in a subfolder.\n",
    " Project Root\n",
    "  |- main.ipynb\n",
    "  |- Geolife Trajectories 1.3/\n",
    "     |- Data/\n",
    "        |- 000/\n",
    "        |- 001/\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54221502",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOLIFE_BASE_PATH = os.path.join('Geolife Trajectories 1.3', 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfed63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster development/demonstration, we can limit the number of users to process.\n",
    "USER_SUBSET = 181 \n",
    "\n",
    "# Parameters for filtering driving trajectories\n",
    "MIN_AVG_SPEED_KMH = 20  # Minimum average speed for a trip 'driving'\n",
    "MAX_ALLOWED_SPEED_KMH = 150 # A sanity check to remove erroneous GPS points\n",
    "\n",
    "# DBSCAN parameters for clustering Origin-Destination points\n",
    "OD_CLUSTER_EPS = 0.5  # Epsilon in km. Two points are neighbors if they are within 0.5 km.\n",
    "OD_CLUSTER_MIN_SAMPLES = 10 # Minimum number of trips to form a dense O-D cluster.\n",
    "\n",
    "# Anomaly detection parameters\n",
    "ANOMALY_DISTANCE_THRESHOLD_METERS = 200 \n",
    "\n",
    "# Output file\n",
    "OUTPUT_MAP_FILE = 'geolife_route_analysis.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a99b8",
   "metadata": {},
   "source": [
    "### 3. Data Ingestion and Preprocessing\n",
    " This section contains functions to parse the raw .plt files, clean the data, and compile it into a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_plt_file(file_path: str, user_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Parses a single .plt file into a DataFrame.\"\"\"\n",
    "    try:\n",
    "        col_names = ['lat', 'lon', 'ignored', 'altitude_ft', 'days_since_1899', 'date', 'time']\n",
    "        df = pd.read_csv(file_path, skiprows=6, header=None, names=col_names, usecols=['lat', 'lon', 'date', 'time'])\n",
    "        if df.empty: return pd.DataFrame()\n",
    "        \n",
    "        df['timestamp'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "        df['user_id'] = user_id\n",
    "        df['trajectory_id'] = os.path.basename(file_path).replace('.plt', '')\n",
    "        return df[['user_id', 'trajectory_id', 'timestamp', 'lat', 'lon']]\n",
    "    except Exception:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ff2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_trajectories(base_path: str, user_limit: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Loads all trajectories from the GeoLife dataset into a single DataFrame.\"\"\"\n",
    "    all_dfs = []\n",
    "    user_folders = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "    \n",
    "    if user_limit:\n",
    "        user_folders = user_folders[:user_limit]\n",
    "        \n",
    "    print(f\"Starting to process {len(user_folders)} users...\")\n",
    "    \n",
    "    for i, user_id in enumerate(user_folders):\n",
    "        user_path = os.path.join(base_path, user_id, 'Trajectory')\n",
    "        if not os.path.exists(user_path): continue\n",
    "        \n",
    "        plt_files = [f for f in os.listdir(user_path) if f.endswith('.plt')]\n",
    "        for traj_file in plt_files:\n",
    "            df = parse_plt_file(os.path.join(user_path, traj_file), user_id)\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "        print(f\"Progress: {i+1}/{len(user_folders)} users processed.\", end='\\r')\n",
    "        \n",
    "    print(\"\\nConcatenating all trajectories...\")\n",
    "    return pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7590c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process 181 users...\n",
      "Progress: 110/181 users processed.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manik\\AppData\\Local\\Temp\\ipykernel_19592\\3820687217.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['timestamp'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 181/181 users processed.\n",
      "Concatenating all trajectories...\n",
      "\n",
      "Data loading complete. Took 641.78 seconds.\n",
      "Loaded 24859853 points from 17727 trajectories.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>trajectory_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "      <td>20090402060732</td>\n",
       "      <td>2009-04-02 06:07:32</td>\n",
       "      <td>40.000102</td>\n",
       "      <td>116.327021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>20090402060732</td>\n",
       "      <td>2009-04-02 06:07:42</td>\n",
       "      <td>40.002471</td>\n",
       "      <td>116.327546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000</td>\n",
       "      <td>20090402060732</td>\n",
       "      <td>2009-04-02 06:07:47</td>\n",
       "      <td>39.999973</td>\n",
       "      <td>116.326985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000</td>\n",
       "      <td>20090402060732</td>\n",
       "      <td>2009-04-02 06:07:52</td>\n",
       "      <td>39.999907</td>\n",
       "      <td>116.327082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000</td>\n",
       "      <td>20090402060732</td>\n",
       "      <td>2009-04-02 06:07:57</td>\n",
       "      <td>39.999883</td>\n",
       "      <td>116.327228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id   trajectory_id           timestamp        lat         lon\n",
       "0     000  20090402060732 2009-04-02 06:07:32  40.000102  116.327021\n",
       "1     000  20090402060732 2009-04-02 06:07:42  40.002471  116.327546\n",
       "2     000  20090402060732 2009-04-02 06:07:47  39.999973  116.326985\n",
       "3     000  20090402060732 2009-04-02 06:07:52  39.999907  116.327082\n",
       "4     000  20090402060732 2009-04-02 06:07:57  39.999883  116.327228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the data loading\n",
    "start_time = time.time()\n",
    "raw_df = load_all_trajectories(GEOLIFE_BASE_PATH, USER_SUBSET)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nData loading complete. Took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Loaded {len(raw_df)} points from {raw_df['trajectory_id'].nunique()} trajectories.\")\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1171fe",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering & Driving Segment Detection\n",
    " We will now process the raw data to calculate speed and filter for trajectories that represent vehicle travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3f726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating GPS coordinates...\n",
      "Removed 1 invalid GPS points.\n",
      "Remaining points: 24859852\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating GPS coordinates...\")\n",
    "points_before_cleaning = len(raw_df)\n",
    "raw_df = raw_df[\n",
    "    (raw_df['lat'].between(-90, 90)) &\n",
    "    (raw_df['lon'].between(-180, 180))\n",
    "]\n",
    "points_after_cleaning = len(raw_df)\n",
    "points_removed = points_before_cleaning - points_after_cleaning\n",
    "print(f\"Removed {points_removed} invalid GPS points.\")\n",
    "print(f\"Remaining points: {points_after_cleaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26242fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating speeds for all trajectories...\n",
      "Speed calculation complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trajectory_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>speed_kmh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22561929</th>\n",
       "      <td>20000101231219</td>\n",
       "      <td>2000-01-01 23:13:21</td>\n",
       "      <td>39.990964</td>\n",
       "      <td>116.327041</td>\n",
       "      <td>12.732507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22561930</th>\n",
       "      <td>20000101231219</td>\n",
       "      <td>2000-01-01 23:15:23</td>\n",
       "      <td>39.993207</td>\n",
       "      <td>116.326827</td>\n",
       "      <td>7.379291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395124</th>\n",
       "      <td>20070412093132</td>\n",
       "      <td>2007-04-12 09:39:37</td>\n",
       "      <td>39.974317</td>\n",
       "      <td>116.330450</td>\n",
       "      <td>0.080677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395125</th>\n",
       "      <td>20070412093132</td>\n",
       "      <td>2007-04-12 09:40:49</td>\n",
       "      <td>39.974467</td>\n",
       "      <td>116.330450</td>\n",
       "      <td>0.833963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395126</th>\n",
       "      <td>20070412093132</td>\n",
       "      <td>2007-04-12 09:43:37</td>\n",
       "      <td>39.974583</td>\n",
       "      <td>116.330450</td>\n",
       "      <td>0.277988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           trajectory_id           timestamp        lat         lon  speed_kmh\n",
       "22561929  20000101231219 2000-01-01 23:13:21  39.990964  116.327041  12.732507\n",
       "22561930  20000101231219 2000-01-01 23:15:23  39.993207  116.326827   7.379291\n",
       "19395124  20070412093132 2007-04-12 09:39:37  39.974317  116.330450   0.080677\n",
       "19395125  20070412093132 2007-04-12 09:40:49  39.974467  116.330450   0.833963\n",
       "19395126  20070412093132 2007-04-12 09:43:37  39.974583  116.330450   0.277988"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_speed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates distance and speed for each point in the trajectories.\n",
    "    This function uses the Haversine formula for accurate distance calculation on a sphere.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['trajectory_id', 'timestamp'])\n",
    "    \n",
    "    # Shift coordinates to get previous point's lat/lon\n",
    "    df['prev_lat'] = df.groupby('trajectory_id')['lat'].shift(1)\n",
    "    df['prev_lon'] = df.groupby('trajectory_id')['lon'].shift(1)\n",
    "    df['prev_timestamp'] = df.groupby('trajectory_id')['timestamp'].shift(1)\n",
    "\n",
    "   \n",
    "    df = df.dropna()\n",
    "\n",
    "    # Calculate distance using geopy's great_circle function (Haversine)\n",
    "    df['distance_km'] = df.apply(\n",
    "        lambda row: great_circle((row['lat'], row['lon']), (row['prev_lat'], row['prev_lon'])).kilometers,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "  \n",
    "    df['time_diff_hr'] = (df['timestamp'] - df['prev_timestamp']).dt.total_seconds() / 3600.0\n",
    "    \n",
    "    \n",
    "    df['speed_kmh'] = df['distance_km'] / df['time_diff_hr'].replace(0, np.nan)\n",
    "    \n",
    "    # Clean up infinite values and NaNs that might result from the calculation\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(subset=['speed_kmh'], inplace=True)\n",
    "    \n",
    "    return df[['trajectory_id', 'timestamp', 'lat', 'lon', 'speed_kmh']]\n",
    "\n",
    "print(\"Calculating speeds for all trajectories...\")\n",
    "speed_df = calculate_speed(raw_df.copy())\n",
    "print(\"Speed calculation complete.\")\n",
    "speed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965ed95",
   "metadata": {},
   "source": [
    "#### Filtering for Driving Trips\n",
    "We apply heuristics on speed to identify trips likely made by car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6745a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for driving trajectories...\n",
      "Filtered down to 4792 driving trajectories.\n",
      "Total driving GPS points: 4643511\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering for driving trajectories...\")\n",
    "\n",
    "trajectory_stats = speed_df.groupby('trajectory_id')['speed_kmh'].agg(['mean', 'max']).reset_index()\n",
    "\n",
    "# Filter based on our defined speed thresholds\n",
    "driving_trajectories_ids = trajectory_stats[\n",
    "    (trajectory_stats['mean'] >= MIN_AVG_SPEED_KMH) & \n",
    "    (trajectory_stats['max'] < MAX_ALLOWED_SPEED_KMH)\n",
    "]['trajectory_id']\n",
    "\n",
    "driving_df = speed_df[speed_df['trajectory_id'].isin(driving_trajectories_ids)].copy()\n",
    "\n",
    "print(f\"Filtered down to {driving_df['trajectory_id'].nunique()} driving trajectories.\")\n",
    "print(f\"Total driving GPS points: {len(driving_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965b39f",
   "metadata": {},
   "source": [
    "### 5. Origin-Destination (O-D) Pair Identification\n",
    " We use DBSCAN clustering on the start and end points of trips to find common O-D zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "580dd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4792 Origin-Destination pairs.\n"
     ]
    }
   ],
   "source": [
    "# Extract start and end points for each driving trajectory\n",
    "od_points = driving_df.groupby('trajectory_id').agg(\n",
    "    start_lat=('lat', 'first'),\n",
    "    start_lon=('lon', 'first'),\n",
    "    end_lat=('lat', 'last'),\n",
    "    end_lon=('lon', 'last')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Extracted {len(od_points)} Origin-Destination pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f8350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering start points with DBSCAN...\n"
     ]
    }
   ],
   "source": [
    "# Cluster the start points\n",
    "print(\"Clustering start points with DBSCAN...\")\n",
    "# Convert epsilon from km to degrees for DBSCAN (approximate)\n",
    "kms_per_radian = 6371.0088\n",
    "eps_in_degrees = OD_CLUSTER_EPS / kms_per_radian\n",
    "\n",
    "dbscan_start = DBSCAN(eps=eps_in_degrees, min_samples=OD_CLUSTER_MIN_SAMPLES, algorithm='ball_tree', metric='haversine')\n",
    "od_points['start_cluster'] = dbscan_start.fit_predict(np.radians(od_points[['start_lat', 'start_lon']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f4834d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering end points with DBSCAN...\n"
     ]
    }
   ],
   "source": [
    "# Cluster the end points\n",
    "print(\"Clustering end points with DBSCAN...\")\n",
    "dbscan_end = DBSCAN(eps=eps_in_degrees, min_samples=OD_CLUSTER_MIN_SAMPLES, algorithm='ball_tree', metric='haversine')\n",
    "od_points['end_cluster'] = dbscan_end.fit_predict(np.radians(od_points[['end_lat', 'end_lon']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3218abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most frequent O-D pair is from Start Cluster 0 to End Cluster 0\n",
      "Number of trips in this pair: 254\n"
     ]
    }
   ],
   "source": [
    "# Filter out trips that don't belong to a cluster (noise points, labeled -1)\n",
    "clustered_od = od_points[(od_points['start_cluster'] != -1) & (od_points['end_cluster'] != -1)]\n",
    "\n",
    "# Identify the most frequent O-D pair\n",
    "if not clustered_od.empty:\n",
    "    od_pair_counts = clustered_od.groupby(['start_cluster', 'end_cluster']).size().reset_index(name='count')\n",
    "    most_frequent_pair = od_pair_counts.sort_values('count', ascending=False).iloc[0]\n",
    "\n",
    "    start_cluster_id = int(most_frequent_pair['start_cluster'])\n",
    "    end_cluster_id = int(most_frequent_pair['end_cluster'])\n",
    "\n",
    "    print(f\"\\nMost frequent O-D pair is from Start Cluster {start_cluster_id} to End Cluster {end_cluster_id}\")\n",
    "    print(f\"Number of trips in this pair: {most_frequent_pair['count']}\")\n",
    "else:\n",
    "    print(\"\\nCould not find any dense O-D clusters with the current settings. Try adjusting DBSCAN parameters or processing more users.\")\n",
    "    start_cluster_id = -1 # Set to invalid to skip subsequent steps\n",
    "    end_cluster_id = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf8ef3",
   "metadata": {},
   "source": [
    "### 6. Normal Route Generation\n",
    " For the most frequent O-D pair, we will now generate a \"normal\" or representative route.\n",
    " A simple and effective method is to find the trajectory that is most \"central\" to all other trajectories in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff4098ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected trajectory 20070416032733 as the 'Normal Route'.\n",
      "Normal route has 32 points.\n"
     ]
    }
   ],
   "source": [
    "if start_cluster_id != -1:\n",
    "    # Get all trajectory IDs for our chosen O-D pair\n",
    "    target_od_trips_ids = clustered_od[\n",
    "        (clustered_od['start_cluster'] == start_cluster_id) & \n",
    "        (clustered_od['end_cluster'] == end_cluster_id)\n",
    "    ]['trajectory_id']\n",
    "\n",
    "    target_od_df = driving_df[driving_df['trajectory_id'].isin(target_od_trips_ids)]\n",
    "\n",
    "    # For simplicity, we'll select one of these trips as our \"normal route\".\n",
    "    # A more advanced method would involve averaging or finding a central path.\n",
    "    normal_route_id = target_od_trips_ids.iloc[0]\n",
    "    normal_route_df = target_od_df[target_od_df['trajectory_id'] == normal_route_id]\n",
    "\n",
    "    print(f\"Selected trajectory {normal_route_id} as the 'Normal Route'.\")\n",
    "    print(f\"Normal route has {len(normal_route_df)} points.\")\n",
    "else:\n",
    "    normal_route_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185e049",
   "metadata": {},
   "source": [
    "### 7. Anomaly Detection\n",
    " Now, we'll take another trip from the same O-D pair and check how much it deviates from our \"normal route\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3fc449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected trajectory 20070425154640 as the 'Test Trip'.\n",
      "Calculating deviation from normal route for test trip...\n",
      "Anomaly detection complete. Found 38 anomalous points.\n"
     ]
    }
   ],
   "source": [
    "# Select a different trip from the same cluster to act as our \"test trip\"\n",
    "if len(target_od_trips_ids) > 1:\n",
    "    test_trip_id = target_od_trips_ids.iloc[1]\n",
    "    test_trip_df = target_od_df[target_od_df['trajectory_id'] == test_trip_id].copy()\n",
    "    print(f\"Selected trajectory {test_trip_id} as the 'Test Trip'.\")\n",
    "\n",
    "    # Use Shapely for efficient distance calculation\n",
    "    # Create a LineString object for the normal route\n",
    "    normal_route_line = LineString(zip(normal_route_df['lon'], normal_route_df['lat']))\n",
    "\n",
    "    # Calculate the minimum distance from each point in the test trip to the normal route line\n",
    "    def calculate_deviation_meters(row):\n",
    "        point = Point(row['lon'], row['lat'])\n",
    "        # .distance returns distance in the units of the coordinates (degrees)\n",
    "        # We need to convert this to meters. This is an approximation.\n",
    "        # A more precise way would be to project to a metric CRS, but this is sufficient.\n",
    "        degree_dist = point.distance(normal_route_line)\n",
    "        # Approx 1 degree = 111 km\n",
    "        return degree_dist * 111000\n",
    "\n",
    "    print(\"Calculating deviation from normal route for test trip...\")\n",
    "    test_trip_df['deviation_m'] = test_trip_df.apply(calculate_deviation_meters, axis=1)\n",
    "\n",
    "    # Flag points as anomalous if their deviation exceeds the threshold\n",
    "    test_trip_df['is_anomaly'] = test_trip_df['deviation_m'] > ANOMALY_DISTANCE_THRESHOLD_METERS\n",
    "    \n",
    "    anomalies_found = test_trip_df['is_anomaly'].sum()\n",
    "    print(f\"Anomaly detection complete. Found {anomalies_found} anomalous points.\")\n",
    "\n",
    "else:\n",
    "    if normal_route_df.empty:\n",
    "        print(\"Skipping anomaly detection because no normal route was generated.\")\n",
    "    else:\n",
    "        print(\"Not enough trips in the selected O-D cluster to perform anomaly detection.\")\n",
    "    test_trip_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661300a",
   "metadata": {},
   "source": [
    "### 8. Interactive Visualization\n",
    " We use Folium to plot the normal route, the test trip, and the detected anomalies on an interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f49993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactive map saved to 'geolife_route_analysis.html'.\n",
      "Open this file in a web browser to explore the results.\n"
     ]
    }
   ],
   "source": [
    "if not test_trip_df.empty:\n",
    "    # Create a map centered on the start of the normal route\n",
    "    map_center = [normal_route_df['lat'].iloc[0], normal_route_df['lon'].iloc[0]]\n",
    "    m = folium.Map(location=map_center, zoom_start=12, tiles='CartoDB dark_matter')\n",
    "\n",
    "    # Plot the Normal Route (blue)\n",
    "    folium.PolyLine(\n",
    "        locations=normal_route_df[['lat', 'lon']].values,\n",
    "        color='blue',\n",
    "        weight=5,\n",
    "        opacity=0.8,\n",
    "        tooltip='Normal Route'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Plot the Test Trip (green)\n",
    "    folium.PolyLine(\n",
    "        locations=test_trip_df[['lat', 'lon']].values,\n",
    "        color='green',\n",
    "        weight=3,\n",
    "        opacity=0.8,\n",
    "        tooltip='Test Trip'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Plot the Anomalous Points (red circles)\n",
    "    anomalous_points = test_trip_df[test_trip_df['is_anomaly']]\n",
    "    for _, row in anomalous_points.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lon']],\n",
    "            radius=6,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            fill_opacity=0.7,\n",
    "            tooltip=f\"Anomaly! Deviation: {row['deviation_m']:.0f}m\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save the map to an HTML file\n",
    "    m.save(OUTPUT_MAP_FILE)\n",
    "    print(f\"\\nInteractive map saved to '{OUTPUT_MAP_FILE}'.\")\n",
    "    print(\"Open this file in a web browser to explore the results.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping map generation as no test trip was available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84596f5",
   "metadata": {},
   "source": [
    "### 9. Scalability Discussion \n",
    "### How This Solution Scales with Apache Spark\n",
    "\n",
    "While this notebook uses Pandas for its implementation, the logic is designed to be scalable to a distributed computing environment like **Apache Spark**, which is essential for handling petabyte-scale vehicle data at a company like Motorq.\n",
    "\n",
    "1.  **Data Ingestion:**\n",
    "    * **Current:** Reads files sequentially into memory.\n",
    "    * **Spark:** Spark can read thousands of files in parallel directly from a distributed file system (like S3 or HDFS) into a Spark DataFrame. The initial parsing logic (`parse_plt_file`) can be converted into a User-Defined Function (UDF) and applied across the cluster.\n",
    "\n",
    "2.  **Feature Engineering (Speed Calculation):**\n",
    "    * **Current:** Uses `groupby().shift()` in Pandas.\n",
    "    * **Spark:** Spark's Window functions (`lag` over a `WindowSpec` partitioned by `trajectory_id` and ordered by `timestamp`) are designed for exactly this type of calculation and will execute it in a distributed manner, avoiding memory bottlenecks.\n",
    "\n",
    "3.  **O-D Clustering:**\n",
    "    * **Current:** Runs DBSCAN on a single machine.\n",
    "    * **Spark:** While traditional DBSCAN is not easily parallelizable, Spark MLlib has implementations of scalable clustering algorithms like K-Means or Bisecting K-Means. For a more direct parallel DBSCAN, custom implementations or third-party libraries built on Spark could be used to handle massive numbers of O-D points.\n",
    "\n",
    "4.  **Anomaly Detection:**\n",
    "    * **Current:** Calculates distance for a single test trip.\n",
    "    * **Spark:** The process of comparing millions of trips against their respective \"normal routes\" can be massively parallelized. A `crossJoin` followed by a UDF to calculate geospatial distance could find deviations across an entire fleet's data simultaneously.\n",
    "\n",
    "By leveraging Spark, this entire pipeline can transition from a proof-of-concept on a single machine to a production-grade system capable of providing real-time insights for millions of vehicles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d3284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
